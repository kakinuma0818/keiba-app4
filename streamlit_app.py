# streamlit_app.py (debug helper + fallback)
import streamlit as st
import requests
from bs4 import BeautifulSoup
import pandas as pd
import re

st.set_page_config(page_title="KEIBA APP - Debug", layout="wide")
st.title("ğŸ KEIBA APP â€” Debug & Fallback Mode")

# basic maps
KEIBAJO_LIST = ["æœ­å¹Œ", "å‡½é¤¨", "ç¦å³¶", "æ–°æ½Ÿ", "æ±äº¬", "ä¸­å±±", "ä¸­äº¬", "äº¬éƒ½", "é˜ªç¥", "å°å€‰"]
KEIBAJO_ID = {"æœ­å¹Œ":1,"å‡½é¤¨":2,"ç¦å³¶":3,"æ–°æ½Ÿ":4,"æ±äº¬":5,"ä¸­å±±":6,"ä¸­äº¬":7,"äº¬éƒ½":8,"é˜ªç¥":9,"å°å€‰":10}

# helper functions
def fetch_url_text(url):
    try:
        r = requests.get(url, headers={"User-Agent":"Mozilla/5.0"}, timeout=15)
        r.encoding = r.apparent_encoding
        return r.status_code, r.text
    except Exception as e:
        return None, f"ERROR: {e}"

def debug_calendar(date_str):
    # date_str: "YYYY-MM-DD"
    year, month, day = date_str.split("-")
    cal_url = f"https://race.netkeiba.com/top/calendar.html?year={year}&month={month}"
    status, text = fetch_url_text(cal_url)
    if status != 200:
        return {"ok": False, "reason": f"calendar fetch failed: status={status}", "url": cal_url}
    soup = BeautifulSoup(text, "html.parser")
    # collect td cells and find those that contain the day number
    results = []
    # try several approaches: td text, div with day, data-day attributes etc.
    # 1) all td
    tds = soup.find_all("td")
    for i, td in enumerate(tds):
        cell_text = td.get_text(" ", strip=True)
        # match exact day at start or standalone
        if re.search(rf"(^|\D){int(day)}(\D|$)", cell_text):
            # collect link texts + inner html
            links = []
            for a in td.find_all("a"):
                links.append({"text": a.get_text(" ", strip=True), "href": a.get("href")})
            results.append({
                "index": i,
                "cell_text": cell_text,
                "inner_html": str(td)[:4000],  # first 4k chars
                "links": links
            })
    # 2) fallback: find any element that contains the day number as separate element
    if not results:
        # search for elements that contain day as an element (class names)
        candidates = soup.find_all(lambda tag: tag.name in ["div","span"] and str(int(day)) in tag.get_text())
        for c in candidates[:20]:
            links = []
            for a in c.find_all("a"):
                links.append({"text": a.get_text(" ", strip=True), "href": a.get("href")})
            results.append({
                "index_desc": c.name,
                "cell_text": c.get_text(" ", strip=True),
                "inner_html": str(c)[:4000],
                "links": links
            })
    return {"ok": True, "url": cal_url, "results": results}

def find_kaisaibi_and_day_from_calendar_html(soup, day_int, keibajo):
    # Try to find strings like "ä¸­äº¬4å›2æ—¥" inside the calendar html
    for a in soup.find_all("a"):
        txt = a.get_text(" ", strip=True)
        if keibajo in txt and re.search(rf"{day_int}\D", txt):
            m = re.search(r"(\d+)å›(\d+)æ—¥", txt)
            if m:
                return int(m.group(1)), int(m.group(2))
    return None, None

# --- UI ---
st.header("æ“ä½œãƒ¢ãƒ¼ãƒ‰")

col1, col2, col3 = st.columns([2,2,1])
with col1:
    date_input = st.date_input("é–‹å‚¬æ—¥ã‚’é¸æŠ", value=None)
with col2:
    keibajo = st.selectbox("ç«¶é¦¬å ´", KEIBAJO_LIST)
with col3:
    race_no = st.number_input("R", min_value=1, max_value=12, value=11)

st.markdown("---")
st.subheader("A: è‡ªå‹•å–å¾—ï¼ˆãƒ‡ãƒãƒƒã‚°å®Ÿè¡Œï¼‰")
if st.button("ãƒ‡ãƒãƒƒã‚°ï¼šã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼å–å¾—ã¨å€™è£œè¡¨ç¤º"):
    if date_input is None:
        st.error("ã¾ãšæ—¥ä»˜ã‚’é¸æŠã—ã¦ä¸‹ã•ã„")
    else:
        date_str = date_input.strftime("%Y-%m-%d")
        st.info(f"calendar URL ã‚’å–å¾—ã—ã¦ã„ã¾ã™ï¼ˆ{date_str}ï¼‰...")
        out = debug_calendar(date_str)
        if not out.get("ok"):
            st.error(out.get("reason"))
            st.write("calendar URL:", out.get("url"))
        else:
            st.write("calendar URL:", out.get("url"))
            results = out.get("results", [])
            if not results:
                st.warning("ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ä¸­ã«è©²å½“ã‚»ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼ˆHTML æ§‹é€ ãŒäºˆæƒ³ã¨ç•°ãªã‚Šã¾ã™ï¼‰ã€‚")
                st.info("æ¬¡ã¯ç”»é¢ã®ã‚¹ã‚¯ã‚·ãƒ§ã‚’é€ã£ã¦ãã ã•ã„ã€ç§ãŒè§£æã—ã¦ã‚»ãƒ¬ã‚¯ã‚¿ã‚’ä½œã‚Šã¾ã™ã€‚")
            else:
                st.success(f"{len(results)} ä»¶ã®å€™è£œã‚»ãƒ«ã‚’ç™ºè¦‹ã€‚ä¸‹ã«è¡¨ç¤ºã—ã¾ã™ã€‚")
                for i, r in enumerate(results):
                    st.markdown(f"### å€™è£œã‚»ãƒ« {i+1}")
                    st.write("cell_text:", r.get("cell_text"))
                    st.write("inner_htmlï¼ˆå…ˆé ­4kæ–‡å­—ï¼‰:")
                    st.code(r.get("inner_html"))
                    st.write("ãƒªãƒ³ã‚¯ä¸€è¦§ï¼ˆtext / hrefï¼‰:")
                    if r.get("links"):
                        st.table(pd.DataFrame(r.get("links")))
                    else:
                        st.write("ãƒªãƒ³ã‚¯ãªã—")

st.markdown("---")
st.subheader("B: race_id ç›´æ¥æŒ‡å®šï¼ˆå›é¿ç”¨ï¼‰")
race_id_input = st.text_input("race_id ã‚’ç›´æ¥å…¥åŠ›ï¼ˆä¾‹: 202507050211ï¼‰", value="")
if st.button("race_id ã§å‡ºé¦¬è¡¨å–å¾—"):
    if not race_id_input:
        st.error("race_id ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
    else:
        url = f"https://race.netkeiba.com/race/shutuba.html?race_id={race_id_input}"
        st.write("ã‚¢ã‚¯ã‚»ã‚¹URL:", url)
        status, text = fetch_url_text(url)
        if status != 200:
            st.error(f"ãƒšãƒ¼ã‚¸å–å¾—å¤±æ•— status={status}")
        else:
            soup = BeautifulSoup(text, "html.parser")
            table = soup.select_one("table.RaceTable01")
            if table is None:
                st.error("å‡ºé¦¬è¡¨ãƒ†ãƒ¼ãƒ–ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ (table.RaceTable01 ãŒå­˜åœ¨ã—ãªã„)")
                st.code(text[:4000])
            else:
                # parse simple columns (attempt)
                rows = table.select("tr")
                data = []
                for row in rows[1:]:
                    cols = [c.get_text(strip=True) for c in row.select("td")]
                    if cols:
                        data.append(cols)
                if data:
                    st.dataframe(pd.DataFrame(data).head(50), use_container_width=True)
                else:
                    st.warning("ãƒ†ãƒ¼ãƒ–ãƒ«ã¯è¦‹ã¤ã‹ã£ãŸãŒãƒ‡ãƒ¼ã‚¿è§£æã«å¤±æ•—ã—ã¾ã—ãŸã€‚HTML ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

st.markdown("---")
st.write("----")
st.caption("â€» ãƒ‡ãƒãƒƒã‚°å®Ÿè¡Œã§ 'inner_html' ã‚’è²¼ã£ã¦ãã‚Œã‚Œã°ç§ãŒãƒ”ãƒ³ãƒã‚¤ãƒ³ãƒˆã§ä¿®æ­£ã—ã¾ã™ã€‚")
